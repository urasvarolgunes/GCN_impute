iteration i
[[0.7 0.  0.  ... 0.  0.  0. ]
 [0.  0.7 0.  ... 0.  0.  0. ]
 [0.  0.  0.7 ... 0.  0.  0. ]
 ...
 [0.  0.  0.  ... 0.7 0.  0. ]
 [0.  0.  0.  ... 0.  0.7 0. ]
 [0.  0.  0.  ... 0.  0.  0.7]]
training: 359 points
validation: 40 points
EPOCH 1
train loss: 0.19912807643413544
val loss: 0.20290453732013702
EPOCH 101
train loss: 0.19027550518512726
val loss: 0.1929868757724762
EPOCH 201
train loss: 0.18808798491954803
val loss: 0.19072209298610687
EPOCH 301
train loss: 0.18731674551963806
val loss: 0.1904255747795105
EPOCH 401
train loss: 0.18657508492469788
val loss: 0.1903810203075409
early stop count: 6
EPOCH 501
train loss: 0.18568408489227295
val loss: 0.19038423895835876
EPOCH 601
train loss: 0.18476034700870514
val loss: 0.1903683841228485
EPOCH 701
train loss: 0.1839219331741333
val loss: 0.19031411409378052
EPOCH 801
train loss: 0.18318119645118713
val loss: 0.19019438326358795
EPOCH 901
train loss: 0.1825154572725296
val loss: 0.18997932970523834
EPOCH 1001
train loss: 0.18188688158988953
val loss: 0.18968850374221802
EPOCH 1101
train loss: 0.18128180503845215
val loss: 0.18930400907993317
EPOCH 1201
train loss: 0.1806926727294922
val loss: 0.18886558711528778
EPOCH 1301
train loss: 0.18011167645454407
val loss: 0.1883876472711563
EPOCH 1401
train loss: 0.17953473329544067
val loss: 0.18789605796337128
EPOCH 1501
train loss: 0.17896506190299988
val loss: 0.18735620379447937
EPOCH 1601
train loss: 0.1784084439277649
val loss: 0.186834916472435
EPOCH 1701
train loss: 0.17786554992198944
val loss: 0.1862962692975998
EPOCH 1801
train loss: 0.17733877897262573
val loss: 0.18578600883483887
EPOCH 1901
train loss: 0.17682908475399017
val loss: 0.18526796996593475
EPOCH 2001
train loss: 0.176334947347641
val loss: 0.18476906418800354
EPOCH 2101
train loss: 0.17586052417755127
val loss: 0.18428455293178558
EPOCH 2201
train loss: 0.1754041463136673
val loss: 0.18382908403873444
EPOCH 2301
train loss: 0.17496486008167267
val loss: 0.18338456749916077
EPOCH 2401
train loss: 0.1745462715625763
val loss: 0.18297137320041656
EPOCH 2501
train loss: 0.174146831035614
val loss: 0.1825461983680725
EPOCH 2601
train loss: 0.17376494407653809
val loss: 0.18215925991535187
EPOCH 2701
train loss: 0.17340253293514252
val loss: 0.18178658187389374
EPOCH 2801
train loss: 0.17306120693683624
val loss: 0.1814265102148056
EPOCH 2901
train loss: 0.17274042963981628
val loss: 0.181102454662323
EPOCH 3001
train loss: 0.17243808507919312
val loss: 0.18078534305095673
EPOCH 3101
train loss: 0.1721574366092682
val loss: 0.18050312995910645
EPOCH 3201
train loss: 0.17189441621303558
val loss: 0.1802297830581665
EPOCH 3301
train loss: 0.17165051400661469
val loss: 0.179988831281662
EPOCH 3401
train loss: 0.17142300307750702
val loss: 0.17973795533180237
EPOCH 3501
train loss: 0.17121312022209167
val loss: 0.17951948940753937
EPOCH 3601
train loss: 0.17101842164993286
val loss: 0.17929139733314514
EPOCH 3701
train loss: 0.1708383411169052
val loss: 0.17912214994430542
EPOCH 3801
train loss: 0.17067348957061768
val loss: 0.1789638251066208
EPOCH 3901
train loss: 0.17052069306373596
val loss: 0.17878645658493042
EPOCH 4001
train loss: 0.17037837207317352
val loss: 0.17866341769695282
EPOCH 4101
train loss: 0.1702493131160736
val loss: 0.17851155996322632
EPOCH 4201
train loss: 0.17012830078601837
val loss: 0.17839466035366058
EPOCH 4301
train loss: 0.17001712322235107
val loss: 0.17825795710086823
EPOCH 4401
train loss: 0.16991378366947174
val loss: 0.17816764116287231
EPOCH 4501
train loss: 0.16981729865074158
val loss: 0.17805953323841095
EPOCH 4601
train loss: 0.1697295606136322
val loss: 0.17794641852378845
EPOCH 4701
train loss: 0.16964617371559143
val loss: 0.1778699904680252
EPOCH 4801
train loss: 0.1695706695318222
val loss: 0.17781119048595428
EPOCH 4901
train loss: 0.16950054466724396
val loss: 0.17774389684200287
2-NN
0.39638318670576733
5-NN
0.4323069403714565
8-NN
0.4430596285434995
10-NN
0.448435972629521
15-NN
0.45405669599217985
iteration i
ss
s000
